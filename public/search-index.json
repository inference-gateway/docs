{"10":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"15":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"16":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"17":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"20":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"30":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"40":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"47":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"50":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"58":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"70":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"80":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"123":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"200":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"333":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"401":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"1000":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"8080":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"8081":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"8082":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"1741879542":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"agenttoagent":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"a2a":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"integration":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"what":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"is":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"key":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"features":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"how":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"works":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"using":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"with":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"the":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"inference":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gateway":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"cli":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"getting":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"started":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"install":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"initialize":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"your":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"project":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"start":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"interactive":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"chat":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"adding":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"agents":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"configuration":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"add":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"remote":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"agent":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"local":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"docker":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"support":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"an":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"environment":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"variables":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"list":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"configured":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"show":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"details":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"for":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"specific":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"remove":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"delegating":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"tasks":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"to":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"tools":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"viewing":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"connected":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"why":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"use":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"available":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"google":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"calendar":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"creating":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"custom":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"required":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"endpoints":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"capabilities":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"schema":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"protocol":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"implementation":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"best":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"practices":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"design":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"security":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"performance":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"related":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"resources":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"supports":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"enabling":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"large":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"language":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"models":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"llms":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"seamlessly":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"coordinate":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"external":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"specialized":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"this":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"powerful":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"feature":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"allows":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"access":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"and":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"utilize":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"wide":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"range":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"of":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"services":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"through":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"standardized":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"interfaces":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"that":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"enables":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"discover":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"communicate":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"multiple":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"simultaneously":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"each":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"can":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"provide":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"called":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"skills":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"llm":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"automatically":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"fulfill":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"user":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"requests":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"automatic":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"discovery":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"discovers":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"their":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"multiagent":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"coordination":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"in":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"single":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"conversation":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"provides":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"domains":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"distributed":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"architecture":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"run":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"as":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"separate":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"scale":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"independently":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"natural":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"users":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"interact":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"naturally":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"while":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"handles":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"standardization":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"based":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"on":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"interoperability":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"mermaid":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"init":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"theme":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"base":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"themevariables":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"primarycolor":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"326ce5":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"primarytextcolor":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"fff":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"linecolor":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"5d8aa8":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"secondarycolor":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"006100":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"flowchart":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"nodespacing":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"rankspacing":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"curve":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"linear":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"graph":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"td":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"request":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"discoverbravailable":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"registry":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"coordinatebrmultiple":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"bremail":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"brcalendar":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"brcalculator":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"brweather":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"results":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"unified":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"response":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"classdef":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"fill":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"9370db":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"stroke":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"strokewidth":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"2px":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"color":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"white":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"32cd32":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"system":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"f5a800":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"black":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"class":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"when":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"makes":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"analysis":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"analyzes":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"are":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"discovered":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"task":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"decomposition":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"broken":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"down":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"into":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"or":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"sequentially":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"result":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"from":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"all":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"integrated":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"coherent":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"way":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"seamless":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"allowing":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"you":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"delegate":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"directly":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"sessions":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"first":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"bash":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"curl":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"fssl":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"https":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"raw":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"githubusercontent":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"sh":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"infer":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"dedicated":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"commands":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"managing":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"command":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"browse":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"at":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"inferencegateway":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"com":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"agentname":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"agenturl":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"oci":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"ociimage":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"keyvalue":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"be":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"two":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"levels":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"projectlevel":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"inferagents":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"}],"yaml":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"current":[{"title":"Agent-To-Agent (A2A) Integration","excerpt":"The Inference Gateway supports **Agent-To-Agent (A2A)** integration, enabling Large Language Models (LLMs) to seamlessly coordinate with external specialized agents. This powerful feature allows LLMs to access and utilize a wide range of external tools and services through standardized agent interfaces.","url":"/a2a"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"api":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"reference":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"url":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"authentication":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"provider":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"completions":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"body":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"streaming":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"proxy":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"example":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"openai":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"completion":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"health":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"check":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"error":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"responses":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"advanced":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"tool":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"visionmultimodal":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"http":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"base64":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"data":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"direct":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"openapi":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"specification":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"restful":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"interacting":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"various":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"providers":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"documents":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"formats":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"structures":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"relative":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"installation":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"by":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"default":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"localhost":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"running":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"locally":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"if":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"enabled":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"auth_enabletrue":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"must":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"include":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"bearer":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"token":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"authorization":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"your_jwt_token":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"jwt":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"issued":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"identity":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"idp":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"specified":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"openid":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"connect":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"settings":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"oidc_issuer_url":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"oidc_client_id":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"etc":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"validates":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"these":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"tokens":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"against":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"authenticate":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"get":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"across":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"v1models":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"status":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"ok":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"contenttype":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"applicationjson":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"object":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"id":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"gpt4":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"model":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"created":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"owned_by":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"served_by":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"claude3opus20240229":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"anthropic":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"meta":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"groq":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"v1modelsprovider":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"where":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"one":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"cohere":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"cloudflare":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"ollama":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"deepseek":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"mistral":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"moonshot":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gpt3":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"5turbo":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"post":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"json":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"ollamadeepseekr1":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"5b":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"name":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"messages":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"array":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"role":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"assistant":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"message":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"sender":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"content":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"hi":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"doing":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"today":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"stream":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"false":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"optional":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"they":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"re":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"generated":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"max_tokens":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"maximum":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"number":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"generate":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"type":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"function":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"string":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"description":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"parameters":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"defining":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"chatcmpl753":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"deepseekr1":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"choices":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"index":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"thinknokay":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"so":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"greeted":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"me":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"said":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"just":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"starting":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"say":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"should":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"respond":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"friendly":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"nnmaybe":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"acknowledge":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"greeting":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"offer":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"my":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"help":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"something":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"since":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"mentioned":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"working":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"math":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"problems":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"solving":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"puzzles":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"ll":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"stick":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"nni":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"want":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"make":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"sure":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"approaching":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"it":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"right":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"not":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"question":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"yet":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"but":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"see":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"more":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"them":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"maybe":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"assistance":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"responding":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"emoji":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"like":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"would":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"nice":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"nthinknnhello":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"finish_reason":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"length":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"usage":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"prompt_tokens":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"completion_tokens":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"total_tokens":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"true":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"streamed":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"server":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"sent":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"events":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"sse":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"developer":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"mozilla":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"objects":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"}],"helpful":[{"title":"API Reference","excerpt":"Inference Gateway provides a RESTful API for interacting with language models from various providers.","url":"/api-reference"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"overview":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"general":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"kubernetes":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"setup":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"document":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"highlevel":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"designed":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"modular":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"extensible":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"easy":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"new":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"fontfamily":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"arial":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"padding":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"client":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"nodes":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"clients":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"v1chatcompletions":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"auth":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"node":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"oidc":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"ig1":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ig2":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ig3":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"h1":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"h2":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"h3":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"h4":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"h5":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"define":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"styles":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"1px":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"apply":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"following":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"diagram":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"shows":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"ingress":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"border":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"subgraph":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"cluster":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"ingressapi":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"service":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"internal":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"internalclients":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"internalagents":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pod1":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pod2":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pod3":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pods":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"pod":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pg1":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"end":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pg2":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"pg3":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"monitoring":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"stack":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"servicemonitor":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"metrics":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"sm":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"scrapes":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"prometheus":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"grafana":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"connection":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"externalproviders":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"placed":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"inside":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"k8s":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"visually":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext1":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext2":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext3":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext4":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext5":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext6":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext7":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext8":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ext9":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"ffffff":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"externalsvc":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"}],"monitor":[{"title":"Architecture Overview","excerpt":"This document provides a high-level overview of the architecture of the Inference Gateway. The Inference-Gateway is designed to be modular and extensible, allowing easy integration of new models and providers.","url":"/architecture-overview"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"flow":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"keycloak":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"prerequisites":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"setting":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"up":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"option":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"manual":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"configure":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"configmap":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"secret":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"helm":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"obtaining":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"password":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"grant":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"testing":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"only":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"credentials":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"servicetoservice":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"making":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"authenticated":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"selfsigned":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"certificates":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"create":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"keycloaks":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"ca":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"certificate":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"mount":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"troubleshooting":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"common":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"issues":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"next":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"steps":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"secure":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"guide":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"focuses":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"popular":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"opensource":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"management":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"solution":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"valid":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"header":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"validated":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"applications":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"processed":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"otherwise":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"unauthorized":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"returned":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"enable":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"set":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"oidc_issuer_urlhttps":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"section":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"detailed":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"integrating":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"www":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"org":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"v24":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"later":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"recommended":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"github":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"v0":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"kubectl":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"iodocstaskstools":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"deployment":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"shdocsintroinstall":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"chart":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"taskfile":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"devinstallation":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"complete":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"clone":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"repository":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"git":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"cd":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"deploy":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"infrastructure":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"postgresql":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"deployinfrastructure":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"admin":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"console":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"username":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"tempadmin":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"output":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"previous":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"test":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"fetchaccesstoken":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"localv1models":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"manually":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"follow":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"official":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"realm":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"log":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"click":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"enter":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"go":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"save":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"page":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"confidential":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"account":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"changes":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"tab":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"copy":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"email":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"userexample":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"verified":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"temporary":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"off":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"update":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"apiversion":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"v1":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"kind":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"metadata":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"namespace":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"auth_enable":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"stringdata":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"oidc_client_secret":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"yourclientsecret":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"opaque":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"upgrade":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"createnamespace":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"envfrom":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"}],"ghcr":[{"title":"Authentication","excerpt":"Inference Gateway supports authentication through OpenID Connect (OIDC), allowing you to secure your API with various identity providers. This guide focuses on setting up authentication with Keycloak, a popular open-source identity and access management solution.","url":"/authentication"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"special":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"script":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"latest":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"version":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"directory":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"download":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"build":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"source":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"quick":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"core":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"essential":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"modes":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"standard":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mode":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"will":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"analyze":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"code":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"propose":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"ask":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"approval":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"before":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"modifying":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"files":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"plan":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"readonly":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"press":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"shifttab":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"switch":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"indicator":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"explores":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"structure":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"without":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"autoaccept":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"yolo":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"twice":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"executes":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"everything":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"immediately":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"interruption":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"switching":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"prompt":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"enabledisable":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"execution":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"manage":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"whitelist":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"safety":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"sandbox":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"file":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"search":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"development":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"storage":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"backends":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"interface":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"shortcuts":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"userdefined":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"format":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"realworld":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"workflows":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"workflow":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"bug":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"investigation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"fix":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"understand":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"issue":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"reads":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"identifies":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"root":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"cause":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"implement":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"return":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"approve":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"modification":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"commit":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"scratch":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"understanding":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"rapid":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"creates":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"writes":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"no":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"interruptions":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"review":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"back":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"refactoring":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"suggested":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"improvements":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"let":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"read":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"fetch":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"relevant":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"tests":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"documentation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"generation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"markdown":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"readme":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"automated":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"autonomously":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"untested":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"functions":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"comprehensive":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"cases":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"runs":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"verify":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"coverage":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"tips":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"beginners":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"power":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"whitelisting":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"allowed":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"protected":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"paths":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"prompts":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"examples":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"autonomous":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"complex":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"cicd":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"debug":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"permission":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"reset":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"reinitialize":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"validate":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"logging":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"gobased":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"commandline":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"rich":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"tui":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"extensive":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"breaking":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"expected":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"until":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"stable":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"zeroconfiguration":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"keys":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"env":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"chatting":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"manages":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"itself":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"ai":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"iteratively":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"execute":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"readwrite":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"web":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"smart":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"configurable":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"realtime":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"diff":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"visualization":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"flexible":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"toggle":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"between":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"during":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"beautiful":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"scrollable":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"syntax":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"highlighting":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"expansion":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"themes":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"installdir":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"home":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"localbin":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"have":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"installed":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"binaries":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"releases":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"signed":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"cosign":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"verification":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"imagestui":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"gif":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"inferconfig":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"configurations":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"resource":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"launch":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"terminal":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"history":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"mouse":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"wheel":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"keyboard":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"expansioncollapse":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"inspection":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"three":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"operational":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"navigation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"shift":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"arrow":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"downup":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"scroll":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"ctrlr":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"expanded":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"view":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"cycle":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"background":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"codebase":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"suggest":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"failing":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"suite":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"operates":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"planning":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"validation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"phases":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"change":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"behaves":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"anytime":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"session":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"normal":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"operation":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"}],"checks":[{"title":"Inference Gateway CLI","excerpt":"The Inference Gateway CLI (`infer`) is a powerful Go-based command-line tool that provides comprehensive access to the Inference Gateway. It features an interactive chat interface with a rich TUI, autonomous agent capabilities, extensive tool integration, and advanced conversation management.","url":"/cli"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"methods":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"context":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"ui":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"debugging":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"variable":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"configmaps":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"secrets":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"telemetry":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"import":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"configtable":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"options":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"adapt":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"needs":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"facilitate":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"apis":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"proper":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"optimal":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"suit":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"different":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"scenarios":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"most":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"deployments":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"kubernetesbased":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"primary":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"method":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"configuring":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"control":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"basic":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"providerspecific":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"rows":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"defaultvalue":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"production":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"enable_vision":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"telemetry_enable":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"opentelemetry":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"tracing":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"send":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"images":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"alongside":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"text":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"disabled":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"image":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"rejected":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"even":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"vision":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"reasons":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"exposes":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"endpoint":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"scraping":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"generates":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"traces":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"collected":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"collectors":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"issuer":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"behavior":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"server_host":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"host":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"server_port":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"port":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"server_read_timeout":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"timeout":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"30s":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"server_write_timeout":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"write":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"server_idle_timeout":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"idle":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"120s":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"server_tls_cert_path":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"tls":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"path":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"server_tls_key_path":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"strongly":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"pem":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"connects":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"thirdparty":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"client_timeout":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"connections":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"per":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"minimum":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"tls12":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"highthroughput":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"consider":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"increasing":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"pool":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"openai_api_url":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"comv1":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"openai_api_key":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"anthropic_api_url":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"anthropic_api_key":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"cohere_api_url":[{"title":"Configuration","excerpt":"import ConfigTable from '../components/ConfigTable';","url":"/configuration"}],"creation":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"certmanager":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"upgrading":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"uninstalling":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"checking":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"sources":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"explains":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"charts":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"v3":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"enabledtrue":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"inferencegatewaytls":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"certpathtotls":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"crt":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"keypathtotls":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"we":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"recommend":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"io":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"automate":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"approach":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"automates":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"issuance":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"renewal":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"already":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"repo":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"jetstack":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"jetstackcertmanager":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"crds":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"clusterissuer":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"encrypt":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"eof":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"iov1":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"letsencryptprod":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"spec":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"acme":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"acmev02":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"letsencrypt":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"orgdirectory":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"youremailexample":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"privatekeysecretref":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"solvers":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"http01":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"nginx":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"annotations":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"ioclusterissuer":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"annotation":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"values":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"existing":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"completely":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"uninstall":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"delete":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"svc":[{"title":"Deployment with Helm","excerpt":"This guide explains how to deploy the Inference Gateway using the official Helm charts.","url":"/deployment"}],"compose":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"processing":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"environments":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"simple":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"minimal":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"ports":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"appsv1":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"replicas":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"selector":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"matchlabels":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"app":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"template":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"labels":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"containers":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"containerport":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"valuefrom":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"secretkeyref":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"llmsecrets":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"openaiapikey":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"including":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"visit":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"openaigpt4o":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"explain":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"compare":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"contrast":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"process":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"visioncapable":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"enable_visiontrue":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"image_url":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"upload":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"wikimedia":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"jpg":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"pixel":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"imagepng":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"supported":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"claude":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"sonnet":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"haiku":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"googlegemini2":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"5flash":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"ollamallava":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"}],"out":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"full":[{"title":"Examples","excerpt":"This page provides examples of how to use Inference Gateway in various scenarios and environments.","url":"/examples"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"learn":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"pull":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"rm":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"checkout":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"openaigpt4omini":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"hello":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"world":[{"title":"Getting Started","excerpt":"Learn how to install and set up Inference Gateway.","url":"/getting-started"}],"ides":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"vscode":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"cursor":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"fully":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"openaicompatible":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"favorite":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"extensions":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"sections":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"instructions":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"continue":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"dev":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"extension":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"follows":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"marketplace":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"visualstudio":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"continuedev":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"open":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"title":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"yourmodelname":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"apibase":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"8080v1":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"apikey":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"yourapikey":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"restart":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"aifirst":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"editor":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"clicking":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"gear":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"icon":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"bottom":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"left":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"corner":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"shortcut":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"cmd":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"mac":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"ctrl":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"windowslinux":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"sidebar":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"select":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"find":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"now":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"operations":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"other":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"aipowered":[{"title":"Integrated Development Environments (IDEs)","excerpt":"Inference Gateway is fully OpenAI-compatible, allowing you to configure it with your favorite IDEs and extensions using standard OpenAI integration settings. The following sections provide instructions for configuring popular IDEs and extensions.","url":"/ides"}],"servers":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"community":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"main":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"link":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"nextlink":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"simplifying":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"sending":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"receiving":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"mixture":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"experts":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"under":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"mit":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"license":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"easily":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"urls":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"tooluse":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"calling":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"extend":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"ready":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"built":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"mind":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"timeouts":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"lightweight":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"includes":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"libraries":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"runtime":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"resulting":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"smaller":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"size":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"binary":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"8mb":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"consumption":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"consume":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"lower":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"footprint":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"well":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"documented":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"guides":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"tested":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"extensively":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"unit":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"maintained":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"actively":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"developed":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"scalable":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"used":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"hpa":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"compliance":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"privacy":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"does":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"collect":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"analytics":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"ensuring":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"selfhosted":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"over":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"try":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"our":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"href":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"gettingstarted":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"guidelink":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"own":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"instance":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"minutes":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"acts":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"intermediary":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"standardizing":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"interactions":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"changing":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"application":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"sophisticated":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"routing":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"fallback":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"mechanisms":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"centralize":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"policies":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"native":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"calls":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"clientside":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"filesystems":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"databases":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"integrations":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"export":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"mcp_enabletrue":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_servers":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"filesystemserver":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"8081mcp":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"searchserver":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"8082mcp":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"recent":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"news":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"about":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"},{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"integrationlink":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"explore":[{"title":"Inference Gateway Documentation","excerpt":"Inference Gateway is a proxy server designed to facilitate access to various","url":"/"}],"middleware":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"expose":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"commaseparated":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"types":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"filesystem":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"time":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"database":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"handling":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"considerations":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"inspector":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"included":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"tutorials":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"python":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"failed":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"network":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"connectivity":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"appearing":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcpspecific":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_exposetrue":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"requiring":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"individually":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"modelcontextprotocol":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"securely":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"systems":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"query":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"manipulate":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"engines":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"retrieve":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"information":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"made":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"multiserver":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"dynamic":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"injection":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"injected":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"executed":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"transparently":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"zero":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"don":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"need":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"know":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"individual":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"builtin":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"observability":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"},{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"tb":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"assembly":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"sends":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"added":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"decides":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"which":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"via":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"delivery":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"timeserver":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"8083mcp":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_dial_timeout5s":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_servershttp":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcptimeserver":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcpsearchserver":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"groq_api_key":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"depends_on":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"deploying":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_enable":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_expose":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_client_timeout":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"10s":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"mcp_request_timeout":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"once":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"txt":[{"title":"Model Context Protocol (MCP) Integration","excerpt":"The Inference Gateway supports **Model Context Protocol (MCP)** integration, enabling seamless access to external tools and data sources for Large Language Models (LLMs). This powerful feature automatically discovers and provides tools to LLMs without requiring clients to manage them individually.","url":"/mcp"}],"collection":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"components":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"prebuilt":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"dashboards":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"robust":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"troubleshoot":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"integrate":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"logs":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"insights":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"telemetry_enabletrue":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"scraped":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"count":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"duration":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"rates":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"utilization":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"cpu":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"memory":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"exposing":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"timeseries":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"storing":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"platform":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"scrape":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"coreos":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"interval":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"15s":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"namespaceselector":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"matchnames":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"visualize":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"volume":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"latency":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"imported":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"outputs":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"structured":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"analyzed":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"elasticsearch":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"loki":[{"title":"Observability","excerpt":"Inference Gateway provides robust observability features to help monitor and troubleshoot your deployment. These features include metrics collection, tracing, and logging capabilities that integrate with popular monitoring tools.","url":"/observability"}],"software":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"kits":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"sdks":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"sdk":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"programming":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"languages":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"simplify":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"typed":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"convenience":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"currently":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"div":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"classname":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"grid":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gridcols1":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"md":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gridcols2":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gap4":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"mb8":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"p4":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"roundedmd":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3pythonh3":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"stronginstallation":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"strong":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"},{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"pip":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"stronggithub":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"strongpypi":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"pypi":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"h3typescripth3":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"typescript":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"npm":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"inferencegatewaysdk":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"strongnpm":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"npmjs":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"h3goh3":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"h3rusth3":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"rust":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"cargo":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"strongcrate":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"crates":[{"title":"Software Development Kits (SDK's)","excerpt":"Inference Gateway provides official SDKs for various programming languages to simplify integration with your applications. These SDKs offer typed interfaces, error handling, and convenience methods for interacting with the Inference Gateway API.","url":"/sdks"}],"h3openaih3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"paccess":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gpt":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"tokendiv":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"divstrongdefault":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"comv1div":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"divstrongvision":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"yes":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gpt4o":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gpt5":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"turbo":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3deepseekh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"puse":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"strongauthentication":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"strongdefault":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"strongvision":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3anthropich3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"pconnect":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"highquality":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"conversational":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"xheader":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3cohereh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"aya":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3groqh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"highperformance":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"lpuaccelerated":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"comopenaiv1":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3cloudflareh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"workers":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"comclientv4accounts":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"textsm":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"account_id":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"aidiv":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3ollamah3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"prun":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"none":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"llava":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"llama":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3googleh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"gemini":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"generativelanguage":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"googleapis":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3mistralh3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"pixtral":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"aiv1":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"h3moonshoth3":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"aiv1div":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"yesdiv":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"several":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"note":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}],"containing":[{"title":"Supported Providers","excerpt":"Inference Gateway provides a unified interface to interact with multiple LLM providers.","url":"/supported-providers"}]}