# Configuration

import ConfigTable from '../components/ConfigTable';

Inference Gateway provides flexible configuration options to adapt to your specific needs. As a proxy server designed to facilitate access to various language model APIs, proper configuration is essential for optimal performance and security.

## Configuration Methods

Inference Gateway supports multiple configuration methods to suit different deployment scenarios:

1. **Environment Variables** - Recommended for most deployments
2. **Kubernetes ConfigMaps and Secrets** - For Kubernetes-based deployments
3. **Configuration Files** - For local development and testing

## Environment Variables

Environment variables are the primary method for configuring Inference Gateway. These variables control everything from basic server settings to provider-specific API configurations.

### General Settings

<ConfigTable
  rows={[
    { variable: 'ENVIRONMENT', description: 'Deployment environment', defaultValue: 'production' },
    {
      variable: 'ENABLE_VISION',
      description: 'Enable vision/multimodal support for all providers',
      defaultValue: 'false',
    },
    {
      variable: 'TELEMETRY_ENABLE',
      description: 'Enable OpenTelemetry metrics and tracing',
      defaultValue: 'false',
    },
    { variable: 'AUTH_ENABLE', description: 'Enable OIDC authentication', defaultValue: 'false' },
  ]}
/>

When `ENABLE_VISION` is set to `true`, Inference Gateway enables vision/multimodal capabilities, allowing you to send images alongside text in chat completion requests. When disabled (default), requests with image content will be rejected even if the provider and model support vision. This is disabled by default for performance and security reasons.

When `TELEMETRY_ENABLE` is set to `true`, Inference Gateway exposes a `/metrics` endpoint for Prometheus scraping and generates distributed traces that can be collected by OpenTelemetry collectors.

### OpenID Connect

If authentication is enabled (`AUTH_ENABLE=true`), configure the following OIDC settings:

<ConfigTable
  rows={[
    {
      variable: 'OIDC_ISSUER_URL',
      description: 'OIDC issuer URL',
      defaultValue: 'http://keycloak:8080/realms/inference-gateway-realm',
    },
    {
      variable: 'OIDC_CLIENT_ID',
      description: 'OIDC client ID',
      defaultValue: 'inference-gateway-client',
    },
    { variable: 'OIDC_CLIENT_SECRET', description: 'OIDC client secret', defaultValue: '""' },
  ]}
/>

When authentication is enabled, all API requests must include a valid JWT token in the Authorization header:

```http
Authorization: Bearer YOUR_JWT_TOKEN
```

### Server Settings

These settings control the core HTTP server behavior:

<ConfigTable
  rows={[
    { variable: 'SERVER_HOST', description: 'Server host', defaultValue: '0.0.0.0' },
    { variable: 'SERVER_PORT', description: 'Server port', defaultValue: '8080' },
    { variable: 'SERVER_READ_TIMEOUT', description: 'Read timeout', defaultValue: '30s' },
    { variable: 'SERVER_WRITE_TIMEOUT', description: 'Write timeout', defaultValue: '30s' },
    { variable: 'SERVER_IDLE_TIMEOUT', description: 'Idle timeout', defaultValue: '120s' },
    { variable: 'SERVER_TLS_CERT_PATH', description: 'TLS certificate path', defaultValue: '""' },
    { variable: 'SERVER_TLS_KEY_PATH', description: 'TLS key path', defaultValue: '""' },
  ]}
/>

For production deployments, it's strongly recommended to configure TLS:

```bash
SERVER_TLS_CERT_PATH=/path/to/certificate.pem
SERVER_TLS_KEY_PATH=/path/to/private-key.pem
```

### Client Settings

These settings control how Inference Gateway connects to third-party APIs:

<ConfigTable
  rows={[
    { variable: 'CLIENT_TIMEOUT', description: 'Client timeout', defaultValue: '30s' },
    {
      variable: 'CLIENT_MAX_IDLE_CONNS',
      description: 'Maximum idle connections',
      defaultValue: '20',
    },
    {
      variable: 'CLIENT_MAX_IDLE_CONNS_PER_HOST',
      description: 'Maximum idle connections per host',
      defaultValue: '20',
    },
    {
      variable: 'CLIENT_IDLE_CONN_TIMEOUT',
      description: 'Idle connection timeout',
      defaultValue: '30s',
    },
    {
      variable: 'CLIENT_TLS_MIN_VERSION',
      description: 'Minimum TLS version',
      defaultValue: 'TLS12',
    },
  ]}
/>

For high-throughput deployments, consider increasing the connection pool settings:

```bash
CLIENT_MAX_IDLE_CONNS=100
CLIENT_MAX_IDLE_CONNS_PER_HOST=50
```

### Provider Settings

Configure access to various LLM providers. At minimum, you should configure the providers you plan to use.

#### OpenAI

<ConfigTable
  rows={[
    {
      variable: 'OPENAI_API_URL',
      description: 'OpenAI API URL',
      defaultValue: 'https://api.openai.com/v1',
    },
    { variable: 'OPENAI_API_KEY', description: 'OpenAI API Key', defaultValue: '""' },
  ]}
/>

#### Anthropic

<ConfigTable
  rows={[
    {
      variable: 'ANTHROPIC_API_URL',
      description: 'Anthropic API URL',
      defaultValue: 'https://api.anthropic.com/v1',
    },
    { variable: 'ANTHROPIC_API_KEY', description: 'Anthropic API Key', defaultValue: '""' },
  ]}
/>

#### Cohere

<ConfigTable
  rows={[
    {
      variable: 'COHERE_API_URL',
      description: 'Cohere API URL',
      defaultValue: 'https://api.cohere.com',
    },
    { variable: 'COHERE_API_KEY', description: 'Cohere API Key', defaultValue: '""' },
  ]}
/>

#### Groq

<ConfigTable
  rows={[
    {
      variable: 'GROQ_API_URL',
      description: 'Groq API URL',
      defaultValue: 'https://api.groq.com/openai/v1',
    },
    { variable: 'GROQ_API_KEY', description: 'Groq API Key', defaultValue: '""' },
  ]}
/>

#### Ollama

<ConfigTable
  rows={[
    {
      variable: 'OLLAMA_API_URL',
      description: 'Ollama API URL',
      defaultValue: 'http://ollama:8080/v1',
    },
    { variable: 'OLLAMA_API_KEY', description: 'Ollama API Key', defaultValue: '""' },
    {
      variable: 'OLLAMA_CLOUD_API_URL',
      description: 'Ollama Cloud API URL',
      defaultValue: 'https://ollama.com/v1',
    },
    { variable: 'OLLAMA_CLOUD_API_KEY', description: 'Ollama Cloud API Key', defaultValue: '""' },
  ]}
/>

#### Cloudflare

<ConfigTable
  rows={[
    {
      variable: 'CLOUDFLARE_API_URL',
      description: 'Cloudflare API URL',
      defaultValue: 'https://api.cloudflare.com/client/v4/accounts/ACCOUNT_ID/ai',
    },
    { variable: 'CLOUDFLARE_API_KEY', description: 'Cloudflare API Key', defaultValue: '""' },
  ]}
/>

#### DeepSeek

<ConfigTable
  rows={[
    {
      variable: 'DEEPSEEK_API_URL',
      description: 'DeepSeek API URL',
      defaultValue: 'https://api.deepseek.com',
    },
    { variable: 'DEEPSEEK_API_KEY', description: 'DeepSeek API Key', defaultValue: '""' },
  ]}
/>

#### Google

<ConfigTable
  rows={[
    {
      variable: 'GOOGLE_API_URL',
      description: 'Google AI API URL',
      defaultValue: 'https://generativelanguage.googleapis.com/v1',
    },
    { variable: 'GOOGLE_API_KEY', description: 'Google AI API Key', defaultValue: '""' },
  ]}
/>

### Model Context Protocol (MCP) Settings

These settings control MCP integration for external tool access:

<ConfigTable
  rows={[
    { variable: 'MCP_ENABLE', description: 'Enable MCP middleware', defaultValue: 'false' },
    {
      variable: 'MCP_EXPOSE',
      description: 'Expose MCP endpoints for debugging',
      defaultValue: 'false',
    },
    {
      variable: 'MCP_SERVERS',
      description: 'Comma-separated list of MCP server URLs',
      defaultValue: '""',
    },
    { variable: 'MCP_CLIENT_TIMEOUT', description: 'MCP client timeout', defaultValue: '10s' },
    { variable: 'MCP_DIAL_TIMEOUT', description: 'MCP dial timeout', defaultValue: '5s' },
    {
      variable: 'MCP_TLS_HANDSHAKE_TIMEOUT',
      description: 'MCP TLS handshake timeout',
      defaultValue: '5s',
    },
    {
      variable: 'MCP_RESPONSE_HEADER_TIMEOUT',
      description: 'MCP response header timeout',
      defaultValue: '5s',
    },
    {
      variable: 'MCP_EXPECT_CONTINUE_TIMEOUT',
      description: 'MCP expect continue timeout',
      defaultValue: '2s',
    },
    { variable: 'MCP_REQUEST_TIMEOUT', description: 'MCP request timeout', defaultValue: '10s' },
  ]}
/>

### Agent-to-Agent (A2A) Settings

These settings control A2A protocol integration for agent coordination:

<ConfigTable
  rows={[
    { variable: 'A2A_ENABLE', description: 'Enable A2A protocol support', defaultValue: 'false' },
    {
      variable: 'A2A_EXPOSE',
      description: 'Expose A2A endpoints for debugging',
      defaultValue: 'false',
    },
    {
      variable: 'A2A_AGENTS',
      description: 'Comma-separated list of A2A agent URLs',
      defaultValue: '""',
    },
    { variable: 'A2A_CLIENT_TIMEOUT', description: 'A2A client timeout', defaultValue: '30s' },
  ]}
/>

### UI Settings

These settings control the Inference Gateway UI:

<ConfigTable
  rows={[
    {
      variable: 'INFERENCE_GATEWAY_URL',
      description: 'The URL of the Inference Gateway server',
      defaultValue: 'http://localhost:8080/v1',
    },
  ]}
/>

### Logging and Debugging

These settings control logging and debugging behavior:

<ConfigTable
  rows={[
    {
      variable: 'LOG_LEVEL',
      description: 'Set logging level (debug, info, warn, error)',
      defaultValue: 'info',
    },
  ]}
/>

## Environment Variable File (.env)

For local development, you can use a `.env` file. Create a file named `.env` in your project root:

```bash
# .env file example
ENVIRONMENT=development
TELEMETRY_ENABLE=false
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
```

## Kubernetes ConfigMaps and Secrets

When deploying in Kubernetes, use ConfigMaps for non-sensitive configuration
and Secrets for API keys and other sensitive information.

### Example ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-gateway-config
data:
  ENVIRONMENT: 'production'
  TELEMETRY_ENABLE: 'true'
  SERVER_HOST: '0.0.0.0'
  SERVER_PORT: '8080'
  SERVER_READ_TIMEOUT: '30s'
  SERVER_WRITE_TIMEOUT: '30s'
  SERVER_IDLE_TIMEOUT: '120s'
```

### Example Secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: inference-gateway-secrets
type: Opaque
data:
  ANTHROPIC_API_KEY: '<base64-encoded-key>'
  COHERE_API_KEY: '<base64-encoded-key>'
  OPENAI_API_KEY: '<base64-encoded-key>'
  OIDC_CLIENT_SECRET: '<base64-encoded-key>'
```

## Complete Configuration Example

Here's a comprehensive example for configuring Inference Gateway in a production environment:

```bash

# General settings
ENVIRONMENT=production
ALLOWED_MODELS=
ENABLE_VISION=false
DEBUG_CONTENT_TRUNCATE_WORDS=10
DEBUG_MAX_MESSAGES=100
# Telemetry
TELEMETRY_ENABLE=false
TELEMETRY_METRICS_PORT=9464
# Model Context Protocol (MCP)
MCP_ENABLE=false
MCP_EXPOSE=false
MCP_SERVERS=
MCP_CLIENT_TIMEOUT=5s
MCP_DIAL_TIMEOUT=3s
MCP_TLS_HANDSHAKE_TIMEOUT=3s
MCP_RESPONSE_HEADER_TIMEOUT=3s
MCP_EXPECT_CONTINUE_TIMEOUT=1s
MCP_REQUEST_TIMEOUT=5s
MCP_MAX_RETRIES=3
MCP_RETRY_INTERVAL=5s
MCP_INITIAL_BACKOFF=1s
MCP_ENABLE_RECONNECT=true
MCP_RECONNECT_INTERVAL=30s
MCP_POLLING_ENABLE=true
MCP_POLLING_INTERVAL=30s
MCP_POLLING_TIMEOUT=5s
MCP_DISABLE_HEALTHCHECK_LOGS=true
# Authentication
AUTH_ENABLE=false
AUTH_OIDC_ISSUER=http://keycloak:8080/realms/inference-gateway-realm
AUTH_OIDC_CLIENT_ID=inference-gateway-client
AUTH_OIDC_CLIENT_SECRET=
# Server settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8080
SERVER_READ_TIMEOUT=30s
SERVER_WRITE_TIMEOUT=30s
SERVER_IDLE_TIMEOUT=120s
SERVER_TLS_CERT_PATH=
SERVER_TLS_KEY_PATH=
# Client settings
CLIENT_TIMEOUT=30s
CLIENT_MAX_IDLE_CONNS=20
CLIENT_MAX_IDLE_CONNS_PER_HOST=20
CLIENT_IDLE_CONN_TIMEOUT=30s
CLIENT_TLS_MIN_VERSION=TLS12
CLIENT_DISABLE_COMPRESSION=true
CLIENT_RESPONSE_HEADER_TIMEOUT=10s
CLIENT_EXPECT_CONTINUE_TIMEOUT=1s
# Providers
ANTHROPIC_API_URL=https://api.anthropic.com/v1
ANTHROPIC_API_KEY=
CLOUDFLARE_API_URL=https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai
CLOUDFLARE_API_KEY=
COHERE_API_URL=https://api.cohere.ai
COHERE_API_KEY=
GROQ_API_URL=https://api.groq.com/openai/v1
GROQ_API_KEY=
OLLAMA_API_URL=http://ollama:8080/v1
OLLAMA_API_KEY=
OLLAMA_CLOUD_API_URL=https://ollama.com/v1
OLLAMA_CLOUD_API_KEY=
OPENAI_API_URL=https://api.openai.com/v1
OPENAI_API_KEY=
DEEPSEEK_API_URL=https://api.deepseek.com
DEEPSEEK_API_KEY=
GOOGLE_API_URL=https://generativelanguage.googleapis.com/v1beta/openai
GOOGLE_API_KEY=
MISTRAL_API_URL=https://api.mistral.ai/v1
MISTRAL_API_KEY=
```

## Configuration Best Practices

1. **API Key Security**: Never commit API keys to version control. Use environment variables or secrets management.
2. **TLS in Production**: Always use TLS in production environments to secure data in transit.
3. **Authentication**: Enable authentication in production environments to control access.
4. **Timeouts**: Adjust timeouts based on your expected workloads and response times from LLM providers.
5. **Monitoring**: Enable telemetry in production for observability and performance tracking.

## Next Steps

Once you've configured Inference Gateway, you might want to:

- Check out the [API Reference](/api-reference) for details on available endpoints
- Explore [SDK options](/sdks) for integrating with your application
- Review [Observability](/observability) options for monitoring and logging
