import Link from 'next/link';

# Inference Gateway Documentation

Inference Gateway is a proxy server designed to facilitate access to various 
language model APIs. It allows users to interact with different language models 
through a unified interface, simplifying the configuration and the process of 
sending requests and receiving responses from multiple LLMs, enabling an easy use 
of Mixture of Experts.

## Key Features

- 📜 **Open Source**: Available under the MIT License.
- 🚀 **Unified API Access**: Proxy requests to multiple language model APIs.
- ⚙️ **Environment Configuration**: Easily configure API keys and URLs.
- 🐳 **Docker Support**: Use Docker and Docker Compose for easy setup.
- ☸️ **Kubernetes Support**: Ready for deployment in Kubernetes environments.
- 📊 **OpenTelemetry Tracing**: Monitor and analyze performance.
- 🛡️ **Production Ready**: Built with configurable timeouts and TLS support.
- 🌿 **Lightweight**: Smaller size binary of ~10.8MB.

## Getting Started

Ready to try Inference Gateway? Follow our <Link href="/getting-started">Getting Started guide</Link> to install
and set up your own instance in minutes.

## How It Works

Inference Gateway acts as an intermediary between your applications and various LLM providers.
By standardizing the API interactions, it allows you to:

- Access multiple LLM providers through a single integration
- Switch between providers without changing application code
- Implement sophisticated routing and fallback mechanisms
- Centralize API key management and security policies

## Community

Inference Gateway is an open-source project maintained by a growing community.
Contributions are welcome on [GitHub](https://github.com/inference-gateway/inference-gateway).
