# Supported Providers

Inference Gateway provides a unified interface to interact with multiple LLM providers. 
This page details each supported provider, their configuration, and usage examples.

- [Supported Providers](#supported-providers)
  - [Available Providers](#available-providers)
  - [Using Providers](#using-providers)
    - [Provider Configuration](#provider-configuration)
    - [API Endpoints](#api-endpoints)
      - [1. Unified Generate API](#1-unified-generate-api)
      - [2. Provider Proxy](#2-provider-proxy)
  - [Provider-Specific Examples](#provider-specific-examples)
    - [OpenAI](#openai)
    - [Anthropic](#anthropic)
    - [Cohere](#cohere)
    - [Groq](#groq)
    - [Cloudflare](#cloudflare)
    - [Ollama](#ollama)
  - [Advanced Features](#advanced-features)
    - [Streaming Responses](#streaming-responses)
    - [Tool Use](#tool-use)
    - [Direct API Proxy](#direct-api-proxy)

## Available Providers

The following LLM providers are currently supported:

<div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-8">
  <div className="border p-4 rounded-md">
    <h3>OpenAI</h3>
    Access GPT models including GPT-3.5, GPT-4, and more.
    <strong>Authentication:</strong> Bearer Token
    <strong>Default URL:</strong> https://api.openai.com
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Anthropic</h3>
    Connect to Claude models for high-quality conversational AI.
    <strong>Authentication:</strong> X-Header
    <strong>Default URL:</strong> https://api.anthropic.com
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Cohere</h3>
    Use Cohere's models for various natural language tasks.
    <strong>Authentication:</strong> Bearer Token
    <strong>Default URL:</strong> https://api.cohere.com
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Groq</h3>
    Access high-performance inference with Groq's LPU-accelerated models.
    <strong>Authentication:</strong> Bearer Token
    <strong>Default URL:</strong> https://api.groq.com
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Cloudflare</h3>
    Connect to Cloudflare Workers AI for inference on various models.
    <strong>Authentication:</strong> Bearer Token
    <strong>Default URL:</strong> https://api.cloudflare.com/client/v4/accounts/{'{ACCOUNT_ID}'}
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Ollama</h3>
    Run open-source models locally or on a self-hosted server.
    <strong>Authentication:</strong> None (optional API key)
    <strong>Default URL:</strong> http://ollama:8080
  </div>
</div>

## Using Providers

### Provider Configuration

Each provider requires specific configuration through environment variables:

- `PROVIDER_API_URL`: The base URL for the provider's API
- `PROVIDER_API_KEY`: The authentication key for the provider

Replace "PROVIDER" with the provider name (uppercase): OPENAI, ANTHROPIC, COHERE, GROQ, CLOUDFLARE, OLLAMA.

### API Endpoints

Inference Gateway offers two main approaches to interact with providers:

#### 1. Unified Generate API

The unified API allows you to generate content with a consistent interface across all providers:

```bash
POST /llms/{provider}/generate
Content-Type: application/json

{
  "model": "MODEL_NAME",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello, world!"
    }
  ]
}
```

#### 2. Provider Proxy

You can also proxy requests directly to the provider's native API:

```bash
POST /proxy/{provider}/{path}
Content-Type: application/json

// Provider-specific request body
```

## Provider-Specific Examples

### OpenAI

Generate content with OpenAI models:

```bash
curl -X POST http://localhost:8080/llms/openai/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello, world!"
      }
    ]
  }'
```

List available models:

```bash
curl http://localhost:8080/llms/openai
```

### Anthropic

Generate content with Anthropic Claude models:

```bash
curl -X POST http://localhost:8080/llms/anthropic/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms."
      }
    ]
  }'
```

List available models:

```bash
curl http://localhost:8080/llms/anthropic
```

### Cohere

Generate content with Cohere models:

```bash
curl -X POST http://localhost:8080/llms/cohere/generate \
  -H "Content-Type: application/json" \\
  -d '{
    "model": "command",
    "messages": [
      {
        "role": "user",
        "content": "Write a short poem about AI."
      }
    ]
  }'
```

### Groq

Generate content with Groq's high-performance models:

```bash
curl -X POST http://localhost:8080/llms/groq/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama2-70b-4096",
    "messages": [
      {
        "role": "user",
        "content": "What are the benefits of quantum computing?"
      }
    ]
  }'
```

### Cloudflare

Generate content with Cloudflare Workers AI:

```bash
curl -X POST http://localhost:8080/llms/cloudflare/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "@cf/meta/llama-2-7b-chat-fp16",
    "messages": [
      {
        "role": "user",
        "content": "Explain how neural networks work."
      }
    ]
  }'
```

### Ollama

Generate content with locally-hosted Ollama models:

```bash
curl -X POST http://localhost:8080/llms/ollama/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "llama2",
    "messages": [
      {
        "role": "user",
        "content": "Write a function to calculate Fibonacci numbers in Python."
      }
    ]
  }'
```

## Advanced Features
      
### Streaming Responses

You can stream responses from supported providers by adding the `stream` or `ssevents` parameter:

```bash
curl -X POST http://localhost:8080/llms/openai/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gpt-4",
    "messages": [
      {
        "role": "user",
        "content": "Write a story about a space explorer."
      }
    ],
    "stream": true
  }'
```

### Tool Use

For providers that support function calling (like OpenAI and Anthropic), you can use the `tools` parameter:

```bash
curl -X POST http://localhost:8080/llms/openai/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gpt-4",
    "messages": [
      {
        "role": "user",
        "content": "What is the weather in Paris?"
      }
    ],
    "tools": [
      {
        "type": "function",
        "name": "get_weather",
        "description": "Get the current weather in a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            }
          },
          "required": ["location"]
        }
      }
    ]
  }'
```

### Direct API Proxy

For more advanced use cases, you can proxy requests directly to the provider's API:

```bash
curl -X POST http://localhost:8080/proxy/openai/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gpt-4",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ]
  }'
```
