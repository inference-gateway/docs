# Supported Providers

Inference Gateway provides a unified interface to interact with multiple LLM providers.
This page details each supported provider, their configuration, and usage examples.

## Available Providers

The following LLM providers are currently supported:

<div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-8">
  <div className="border p-4 rounded-md">
    <h3>OpenAI</h3>
    <p>Access GPT models including GPT-3.5, GPT-4, and more.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://api.openai.com/v1</p>
    <p><strong>Vision Support:</strong> ✅ Yes (GPT-4o, GPT-5, GPT-4.1, GPT-4 Turbo)</p>
  </div>

<div className="border p-4 rounded-md">
  <h3>DeepSeek</h3>
  <p>Use DeepSeek's models for various natural language tasks.</p>
  <p>
    <strong>Authentication:</strong> Bearer Token
  </p>
  <p>
    <strong>Default URL:</strong> https://api.deepseek.com
  </p>
  <p>
    <strong>Vision Support:</strong> ❌ No
  </p>
</div>

<div className="border p-4 rounded-md">
  <h3>Anthropic</h3>
  <p>Connect to Claude models for high-quality conversational AI.</p>
  <p>
    <strong>Authentication:</strong> X-Header
  </p>
  <p>
    <strong>Default URL:</strong> https://api.anthropic.com/v1
  </p>
  <p>
    <strong>Vision Support:</strong> ✅ Yes (Claude 3, Claude 4, Claude 4.5 Sonnet, Claude 4.5
    Haiku)
  </p>
</div>

<div className="border p-4 rounded-md">
  <h3>Cohere</h3>
  <p>Use Cohere's models for various natural language tasks.</p>
  <p>
    <strong>Authentication:</strong> Bearer Token
  </p>
  <p>
    <strong>Default URL:</strong> https://api.cohere.com
  </p>
  <p>
    <strong>Vision Support:</strong> ✅ Yes (Command A Vision, Aya Vision)
  </p>
</div>

<div className="border p-4 rounded-md">
  <h3>Groq</h3>
  <p>Access high-performance inference with Groq's LPU-accelerated models.</p>
  <p>
    <strong>Authentication:</strong> Bearer Token
  </p>
  <p>
    <strong>Default URL:</strong> https://api.groq.com/openai/v1
  </p>
  <p>
    <strong>Vision Support:</strong> ✅ Yes (vision models)
  </p>
</div>

<div className="border p-4 rounded-md">
  <h3>Cloudflare</h3>
  <p>Connect to Cloudflare Workers AI for inference on various models.</p>
  <p>
    <strong>Authentication:</strong> Bearer Token
  </p>
  <p>
    <strong>Default URL:</strong> https://api.cloudflare.com/client/v4/accounts/
  </p>
  <p className="text-sm">{'{ACCOUNT_ID}'}/ai</p>
  <p>
    <strong>Vision Support:</strong> ❌ No
  </p>
</div>

<div className="border p-4 rounded-md">
  <h3>Ollama</h3>
  <p>Run open-source models locally or on a self-hosted server.</p>
  <p>
    <strong>Authentication:</strong> None (optional API key)
  </p>
  <p>
    <strong>Default URL:</strong> http://ollama:8080/v1
  </p>
  <p>
    <strong>Vision Support:</strong> ✅ Yes (LLaVA, Llama 4, Llama 3.2 Vision)
  </p>
</div>

  <div className="border p-4 rounded-md">
    <h3>Google</h3>
    <p>Access Google's Gemini models for text generation and understanding.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://generativelanguage.googleapis.com/v1</p>
    <p><strong>Vision Support:</strong> ✅ Yes (Gemini 2.5)</p>
  </div>
</div>

## Vision/Multimodal Support

Several providers support vision/multimodal capabilities, allowing you to process images alongside text. To use vision features, you must enable them in your configuration:

```bash
ENABLE_VISION=true
```

**Note:** Vision support is disabled by default for performance and security reasons. When disabled, requests containing image content will be rejected even if the model supports vision.

### Providers with Vision Support

- **OpenAI**: GPT-4o, GPT-5, GPT-4.1, GPT-4 Turbo
- **Anthropic**: Claude 3, Claude 4, Claude 4.5 Sonnet, Claude 4.5 Haiku
- **Google**: Gemini 2.5
- **Cohere**: Command A Vision, Aya Vision
- **Ollama**: LLaVA, Llama 4, Llama 3.2 Vision
- **Groq**: Vision models
- **Mistral**: Pixtral

### Example Vision Request

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-3-5-sonnet-20241022",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What is in this image?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "https://example.com/image.jpg"
            }
          }
        ]
      }
    ]
  }'
```

## Using Providers

### Provider Configuration

Each provider requires specific configuration through environment variables:

- `PROVIDER_API_URL`: The base URL for the provider's API
- `PROVIDER_API_KEY`: The authentication key for the provider

Replace "PROVIDER" with the provider name (uppercase): OPENAI, ANTHROPIC, COHERE, GROQ, CLOUDFLARE, OLLAMA, GOOGLE, DEEPSEEK.

### API Endpoints

Inference Gateway offers two main approaches to interact with providers:

#### 1. Unified Generate API

The unified API allows you to generate content with a consistent interface across all providers:

```http
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "MODEL_NAME",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello, world!"
    }
  ]
}
```

#### 2. Provider Proxy

You can also proxy requests directly to the provider's native API:

```bash
POST /proxy/{provider}/{path}
Content-Type: application/json

// Provider-specific request body
```

## Provider-Specific Examples

### OpenAI Provider

Generate content with OpenAI models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello, world!"
      }
    ]
  }'
```

List all available models:

```bash
curl http://localhost:8080/v1/models
```

### DeepSeek Provider

Generate content with DeepSeek models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=deepseek \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-reasoner",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "What is the capital of France?"
      }
    ]
  }'
```

List available models:

```bash
curl http://localhost:8080/v1/models?provider=deepseek
```

### Anthropic Provider

Generate content with Anthropic Claude models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms."
      }
    ]
  }'
```

List available models:

```bash
curl http://localhost:8080/v1/models?provider=anthropic
```

### Cohere Provider

Generate content with Cohere models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=cohere \
  -H "Content-Type: application/json" \
  -d '{
    "model": "command",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Write a short poem about AI."
      }
    ]
  }'
```

### Groq Provider

Generate content with Groq's high-performance models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=groq \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2-70b-4096",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "What are the benefits of quantum computing?"
      }
    ]
  }'
```

### Cloudflare Provider

Generate content with Cloudflare Workers AI:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=cloudflare \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.1-8b-instruct",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain how neural networks work."
      }
    ]
  }'
```

### Ollama Provider

Generate content with locally-hosted Ollama models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=ollama \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Write a function to calculate Fibonacci numbers in Python."
      }
    ]
  }'
```

### Google Provider

Generate content with Google's Gemini models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=google \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/gemini-pro",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain the concept of machine learning in simple terms."
      }
    ]
  }'
```
