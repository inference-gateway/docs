# Supported Providers

Inference Gateway provides a unified interface to interact with multiple LLM providers. 
This page details each supported provider, their configuration, and usage examples.

- [Supported Providers](#supported-providers)
  - [Available Providers](#available-providers)
  - [Using Providers](#using-providers)
    - [Provider Configuration](#provider-configuration)
    - [API Endpoints](#api-endpoints)
      - [1. Unified Generate API](#1-unified-generate-api)
      - [2. Provider Proxy](#2-provider-proxy)
  - [Provider-Specific Examples](#provider-specific-examples)
    - [OpenAI](#openai)
    - [Anthropic](#anthropic)
    - [Cohere](#cohere)
    - [Groq](#groq)
    - [Cloudflare](#cloudflare)
    - [Ollama](#ollama)

## Available Providers

The following LLM providers are currently supported:

<div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-8">
  <div className="border p-4 rounded-md">
    <h3>OpenAI</h3>
    <p>Access GPT models including GPT-3.5, GPT-4, and more.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://api.openai.com</p>
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Anthropic</h3>
    <p>Connect to Claude models for high-quality conversational AI.</p>
    <p><strong>Authentication:</strong> X-Header</p>
    <p><strong>Default URL:</strong> https://api.anthropic.com</p>
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Cohere</h3>
    <p>Use Cohere's models for various natural language tasks.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://api.cohere.com</p>
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Groq</h3>
    <p>Access high-performance inference with Groq's LPU-accelerated models.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://api.groq.com</p>
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Cloudflare</h3>
    <p>Connect to Cloudflare Workers AI for inference on various models.</p>
    <p><strong>Authentication:</strong> Bearer Token</p>
    <p><strong>Default URL:</strong> https://api.cloudflare.com/client/v4/accounts/{'{ACCOUNT_ID}'}</p>
  </div>
  
  <div className="border p-4 rounded-md">
    <h3>Ollama</h3>
    <p>Run open-source models locally or on a self-hosted server.</p>
    <p><strong>Authentication:</strong> None (optional API key)</p>
    <p><strong>Default URL:</strong> http://ollama:8080</p>
  </div>
</div>

## Using Providers

### Provider Configuration

Each provider requires specific configuration through environment variables:

- `PROVIDER_API_URL`: The base URL for the provider's API
- `PROVIDER_API_KEY`: The authentication key for the provider

Replace "PROVIDER" with the provider name (uppercase): OPENAI, ANTHROPIC, COHERE, GROQ, CLOUDFLARE, OLLAMA.

### API Endpoints

Inference Gateway offers two main approaches to interact with providers:

#### 1. Unified Generate API

The unified API allows you to generate content with a consistent interface across all providers:

```http
POST /v1/chat/completions
Content-Type: application/json

{
  "model": "MODEL_NAME",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello, world!"
    }
  ]
}
```

#### 2. Provider Proxy

You can also proxy requests directly to the provider's native API:

```bash
POST /proxy/{provider}/{path}
Content-Type: application/json

// Provider-specific request body
```

## Provider-Specific Examples

### OpenAI

Generate content with OpenAI models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello, world!"
      }
    ]
  }'
```

List all available models:

```bash
curl http://localhost:8080/v1/models
```

### Anthropic

Generate content with Anthropic Claude models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-opus-20240229",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms."
      }
    ]
  }'
```

List available models:

```bash
curl http://localhost:8080/v1/models?provider=anthropic
```

### Cohere

Generate content with Cohere models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=cohere \
  -H "Content-Type: application/json" \
  -d '{
    "model": "command",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Write a short poem about AI."
      }
    ]
  }'
```

### Groq

Generate content with Groq's high-performance models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=groq \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2-70b-4096",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "What are the benefits of quantum computing?"
      }
    ]
  }'
```

### Cloudflare

Generate content with Cloudflare Workers AI:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=cloudflare \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3.1-8b-instruct",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain how neural networks work."
      }
    ]
  }'
```

### Ollama

Generate content with locally-hosted Ollama models:

```bash
curl -X POST http://localhost:8080/v1/chat/completions?provider=ollama \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Write a function to calculate Fibonacci numbers in Python."
      }
    ]
  }'
```
