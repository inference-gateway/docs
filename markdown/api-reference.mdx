# API Reference

Inference Gateway provides a RESTful API for interacting with language models from various providers.
This reference documents all available endpoints, request formats, and response structures.

## Table of Contents
- [API Reference](#api-reference)
  - [Table of Contents](#table-of-contents)
  - [Base URL](#base-url)
  - [Authentication](#authentication)
  - [API Endpoints](#api-endpoints)
    - [List All Models](#list-all-models)
    - [List Provider Models](#list-provider-models)
    - [Chat Completions](#chat-completions)
    - [Request Body](#request-body)
    - [Streaming Response](#streaming-response)
    - [Proxy Requests](#proxy-requests)
      - [Example: OpenAI Chat Completion](#example-openai-chat-completion)
    - [Health Check](#health-check)
  - [Error Responses](#error-responses)
  - [Advanced Features](#advanced-features)
    - [Streaming Responses](#streaming-responses)
    - [Tool Use](#tool-use)
    - [Direct API Proxy](#direct-api-proxy)
  - [OpenAPI Specification](#openapi-specification)

## Base URL

All API endpoints are relative to your Inference Gateway installation URL.
By default, this is `http://localhost:8080` when running locally.

## Authentication

If authentication is enabled (`ENABLE_AUTH=true`), all requests must include a bearer token:

```http
Authorization: Bearer YOUR_JWT_TOKEN
```

The JWT token is issued by the configured Identity Provider (IdP) as specified in the OpenID Connect settings 
(`OIDC_ISSUER_URL`, `OIDC_CLIENT_ID`, etc.). Inference Gateway validates these tokens 
against the IdP to authenticate requests.

## API Endpoints

### List All Models

Get a list of all available language models across all configured providers.

```http
GET /v1/models
```

**Response**:

```http
Status: 200 OK
Content-Type: application/json

{
  "object": "list",
  "data": [
    {
      "id": "gpt-4",
      "object": "model",
      "created": 1741879542,
      "owned_by": "openai",
      "served_by": "openai",
    },
    {
      "id": "claude-3-opus-20240229",
      "object": "model",
      "created": 1741879542,
      "owned_by": "anthropic",
      "served_by": "anthropic",
    },
    {
      "id": "deepseek-r1-distill-llama-70b",
      "object": "model",
      "created": 1741879542,
      "owned_by": "Meta",
      "served_by": "groq",
    }
    ...
  ]
}
```

### List Provider Models

Get a list of available models for a specific provider.

```http
GET /v1/models?provider={provider}
```

where `{provider}` is one of: `openai`, `anthropic`, `cohere`, `groq`, `cloudflare`, `ollama`.

**Response**:

```http
Status: 200 OK
Content-Type: application/json

{
  "provider": "openai",
  "object": "list",
  "data": [
    {
      "id": "gpt-4",
      "object": "model",
      "created": 1741879542,
      "owned_by": "openai",
      "served_by": "openai",
    },
    {
      "id": "gpt-3.5-turbo",
      "object": "model",
      "created": 1741879542,
      "owned_by": "openai",
      "served_by": "openai",
    }
  ]
}
```

### Chat Completions

Chat Completions using a specific provider's language model.

```http
POST /v1/chat/completions?provider={provider}
```
### Request Body

```json
{
  "model": "ollama/deepseek-r1:1.5b",   // Model name (e.g., "gpt-4", "claude-3-opus-20240229")
  "messages": [        // Array of messages in the conversation
    {
      "role": "system" | "user" | "assistant" | "tool", // Role of the message sender
      "content": "Hi, how are you doing today?"         // Message content
    }
  ],
  "stream": false,     // Optional: Stream tokens as they're generated (default: false) 
  "max_tokens": 1000,  // Optional: Maximum number of tokens to generate
  "tools": [           // Optional: Tools the model can use
    {
      "type": "function",
      "name": "string",
      "description": "string",
      "parameters": {
        // JSON Schema object defining function parameters
      }
    }
  ]
}
```
**Response**:

```http
Status: 200 OK
Content-Type: application/json

{
  "id": "chatcmpl-753",
  "object": "chat.completion",
  "created": 1741879542,
  "model": "deepseek-r1:1.5b",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "<think>\nOkay, so the user greeted me and said \"Hi, how are you doing today?\" They're just starting to say that. I should respond in a friendly way.\n\nMaybe I can acknowledge their greeting and offer my help with something. Since they mentioned working on math problems or solving puzzles, I'll stick to that.\n\nI want to make sure I'm approaching it the right way. It's not a question yet, but if I see more of them, maybe I can offer more assistance. So responding with an emoji like ðŸ˜Š would be nice.\n</think>\n\nHello! How are you doing today? ðŸ˜Š",
        "role": "assistant"
      },
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 40,
    "completion_tokens": 40,
    "total_tokens": 80
  }
}
```

### Streaming Response

When `stream: true` is specified, responses are streamed as [Server Sent Events(SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events) objects:

```http
data: {"role":"assistant"}

data: {}

data: {"content":"This "}

data: {"content":"is "}

data: {"content":"the "}

data: {}
```

On the final message the usage metrics of the completion are sent.

### Proxy Requests

Pass requests directly through to provider APIs.

```http
{METHOD} /proxy/{provider}/{path}
```

Where:

- `{METHOD}` is any HTTP method (GET, POST, PUT, DELETE, PATCH)
- `{provider}` is one of the supported providers
- `{path}` is the path to proxy to the provider API


#### Example: OpenAI Chat Completion

```http
POST /proxy/openai/v1/chat/completions
Content-Type: application/json

{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Hello! How can I assist you today?"
    }
  ],
  "temperature": 0.7
}
```

### Health Check

Check if the Inference Gateway service is running.

```http
GET /health
```

**Response**:

```http
Status: 200 OK
```

## Error Responses

When an error occurs, the API returns an appropriate HTTP status code with an error message:

```http
{
  "error": "Error message description"
}
```

Common error status codes

- `400 Bad Request` - Invalid request parameters
- `401 Unauthorized` - Missing or invalid authentication
- `500 Internal Server Error` - Server-side error

## Advanced Features
      
### Streaming Responses

You can stream responses from supported providers by setting the `stream` parameter to `true`:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Write a story about a space explorer."
      }
    ],
    "stream": true
  }'
```

### Tool Use

For providers that support function calling (like OpenAI and Anthropic), you can use the `tools` parameter:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "What is the weather in Paris?"
      }
    ],
    "tools": [
      {
        "type": "function",
        "name": "get_weather",
        "description": "Get the current weather in a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            }
          },
          "required": ["location"]
        }
      }
    ]
  }'
```

### Direct API Proxy

For more advanced use cases, you can proxy requests directly to the provider's API:

```bash
curl -X POST http://localhost:8080/proxy/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

## OpenAPI Specification

For a complete API reference in OpenAPI format, you can view the 
<a href="https://github.com/inference-gateway/inference-gateway/blob/main/openapi.yaml" target="_blank" rel="noopener noreferrer"> OpenAPI specification file</a>
